Business Analytics (BA) – is the act of bringing quantitative data to bear on decision making.
In simple terms --- is data visualization and reporting for understand “what happened and what is happening.
Benefits:
1. More Informed Decision-Making - data-driven decision-making
2. Improved Operational Efficiency

How BI based decision should be taken?
	A business decision should always be backed by data
	How to Collect this data?  ---- By identifying right KPIs
	What is a good KPI for business analysts?
		Agreed - all stakeholders should agree on the business analysts’ KPIs.
		Calibrated - scaled with expectations, scope and goals.
		Proper to the business environment - a KPI should be applicable to the relevant context of usage 
		Relevant to the assessment purpose - a KPI target purpose should be clearly stated - to predict  or to track  the business analysts’ work.
		Weighted – KPIs should have its own weight and priority may not weigh equally.
	
	How often the KPIs should be consumed and refined?
	There are two main cycles of consuming and refining KPIs:
		each deliverable should end up with retrospection of the achieved
		
		
automation vs autonomous:
	Automation refers to the ability of a system to control. Ex- a vehicle, like autopilot or cruise control.
	Autonomous is the ability of a system to not only control a vehicle but respond to unexpected hazards. An autonomous system, learns and adapts to dynamic environments, and evolves as the environment around it changes. 
	
4 phases of data analytics:
	analytics 1.0
	big data - 2.0
	big data + IOT - 3.0
	Autonomous - 4.0
	
Q. How to do data analytics - By gathering data --- which should be based on market study
How Facebook uses Data Science? ------- https://data-flair.training/blogs/data-science-at-facebook/

Read case study - 
	CDMA + relience 
	AArogya setu app 
	
Descriptive analysis:
	- What happeded?
	- what is happening?
	Tools:
		Business reporting
		dashboard
		scoreboard
		data warehousing
	Outcome:
		Well defined business problems and opportunities.
		
Predictive analysis:
	- What will happen?
	- Why will it happen?
	Tools:
		Data Mining
		Text mining
		forecasting
	Outcome:
		Accurate projections of future events and outcomes.
		
Prescriptive analytics:
	- making likey forecasting
	- What we do moving forward?
	Tools:
		Optimization
		simulation
		decision modeling
		expert systems
	Outcome:
		Best possible decision and action

What is Data Warehousing?
	A Data Warehousing (DW) is process for collecting and managing data from varied sources to provide meaningful business insights.
	A Data warehouse is typically used to connect and analyze business data from heterogeneous sources. 

Properties of data warehousing:
	1. subject oriented - data warehousing process is proposed to handle with a specific theme which is more defined. These themes can be sales, distributions, marketing etc.
	2. Integrated – It is somewhere same as subject orientation which is made in a reliable format.  Integration means founding a shared entity to scale the all similar data from the different databases. The data also required to be resided into various data warehouse in shared and generally granted manner.
	3. Time-Variant – In this data is maintained via different intervals of time such as weekly, monthly, or annually etc. It founds various time limit which are structured between the large datasets and are held in online transaction process (OLTP). The time limits for data warehouse is wide-ranged than that of operational systems.
	4. Non-Volatile – As the name defines the data resided in data warehouse is permanent. It also means that data is not erased or deleted when new data is inserted. It includes the mammoth quantity of data that is inserted into modification between the selected quantity on logical business. It evaluates the analysis within the technologies of warehouse.
	
language used for data manupulation called - DML - data manupulation language 

Database vs data warehouse
	- Database is a collection of related data that represents some elements of the real world whereas Data warehouse is an information system that stores historical and commutative data from single or multiple sources.
	- Database is designed to record data whereas the Data warehouse is designed to analyze data.
	- Database is application-oriented-collection of data whereas Data Warehouse is the subject-oriented collection of data.
	- Database uses Online Transactional Processing (OLTP) whereas Data warehouse uses Online Analytical Processing (OLAP).
	- Database tables and joins are complicated because they are normalized whereas Data Warehouse tables and joins are easy because they are denormalized.
	- ER modeling techniques are used for designing Database whereas data modeling techniques are used for designing Data Warehouse.
	
oltp vs opal:
	- oltp is an online transactional system. It manages database modification. OLAP is an online analysis and data retrieving process.
	- OLAP creates a single platform for all type of business analysis needs which includes planning, budgeting, forecasting, and analysis while OLTP is useful to administer day to day transactions of an organization.
	- OLAP is characterized by a large volume of data while OLTP is characterized by large numbers of short online transactions.
	- In OLAP, data warehouse is created uniquely so that it can integrate different data sources for building a consolidated database whereas OLTP uses traditional DBMS.
	
Pyramid Diagram of Organizational levels and information requirements:
	for detail reading - https://www.guru99.com/mis-types-information-system.html
	3layers:
	top layer - ESS - Executive support system - this data always unstructured 
	middle layer - MIS/DIS - management support system/decision support system - data is semi structured.
	bottom layer - TPS - Transaction Processing System - data is highly structured

What is OLAP?
	Online Analytical Processing (OLAP) is a category of software that allows users to analyze information from multiple database systems at the same time. It is a technology that enables analysts to extract and view business data from different points of view.
		
OLAP cube:
	for details - 
	https://www.guru99.com/online-analytical-processing.html
	https://galaktika-soft.com/blog/olap-cubes.html
	1. At the core of the OLAP concept, is an OLAP Cube. The OLAP cube is a data structure optimized for very quick data analysis.
	2. The OLAP Cube consists of numeric facts called measures which are categorized by dimensions. OLAP Cube is also called the hypercube.
	3. Usually, data operations and analysis are performed using the simple spreadsheet, where data values are arranged in row and column format. This is ideal for two-dimensional data. However, OLAP contains multidimensional data, with data usually obtained from a different and unrelated source. Using a spreadsheet is not an optimal option. The cube can store and analyze multidimensional data in a logical and orderly manner.
	
How does it work?
	A Data warehouse would extract information from multiple data sources and formats like text files, excel sheet, multimedia files, etc.
	The extracted data is cleaned and transformed. Data is loaded into an OLAP server (or OLAP cube) where information is pre-calculated in advance for further analysis.	
	
Basic analytical operations of OLAP
	Four types of analytical OLAP operations are:
		1. Roll-up
		2. Drill-down
		3. Slice 
		4. dice
		
	1) Roll-up:
	Roll-up is also known as “consolidation” or “aggregation.” The Roll-up operation can be performed in 2 ways
		1. Reducing dimensions
		2. Climbing up concept hierarchy. Concept hierarchy is a system of grouping things based on their order or level.
		
	NOTE: In the roll-up process at least one or more dimensions need to be removed.

	2) Drill-down:
	In drill-down data is fragmented into smaller parts. It is the opposite of the rollup process. It can be done via
		- Moving down the concept hierarchy
		- Increasing a dimension

	NOTE: In the roll-up process at least one or more dimensions need to be added.
	
	3) Slice:
	Here, one dimension is selected, and a new sub-cube is created. achieved by cutting horizontally.
	
	4) Dice:
	This operation is similar to a slice. The difference in dice is you select 2 or more dimensions that result in the creation of a sub-cube.
	
What is Data Mining?
	More details - https://www.guru99.com/data-mining-tutorial.html
	- Data Mining is a process of finding potentially useful patterns from huge data sets. It is a multi-disciplinary skill that uses machine learning, statistics, and AI to extract information to evaluate future events probability. The insights derived from Data Mining are used for marketing, fraud detection, scientific discovery, etc. 
	- Data Mining is all about discovering hidden, unsuspected, and previously unknown yet valid relationships amongst the data. Data mining is also called Knowledge Discovery in Data (KDD), Knowledge extraction, data/pattern analysis, information harvesting, etc.
	
Difference Between Data Mining and Statistics:

	Data mining: Data mining is the method of analyzing expansive sums of data in an exertion to discover relationships, designs, and insights. These designs, concurring to Witten and Eibemust be “meaningful in that they lead to a few advantages, more often than not a financial advantage.” Data in data mining is additionally ordinarily quantitative particularly when we consider the exponential development in data delivered by social media later a long time, i.e. big-data.

	Statistics: Statistics is the science of collecting, organizing, summarizing, and analyzing data to draw conclusions or reply questions. In expansion, measurements are around giving a degree of certainty in any conclusions. The practice or science of collecting and analyzing numerical information in huge amounts, particularly for the reason of gathering extents in entirety from those in a representative test.

Data mining Examples:
	Now in this Data Mining course, let’s learn about Data mining with examples:

	Example 1:

	Consider a marketing head of telecom service provides who wants to increase revenues of long distance services. For high ROI on his sales and marketing efforts customer profiling is important. He has a vast data pool of customer information like age, gender, income, credit history, etc. But its impossible to determine characteristics of people who prefer long distance calls with manual analysis. Using data mining techniques, he may uncover patterns between high long distance call users and their characteristics.

	For example, he might learn that his best customers are married females between the age of 45 and 54 who make more than $80,000 per year. Marketing efforts can be targeted to such demographic.

	Example 2:

	A bank wants to search new ways to increase revenues from its credit card operations. They want to check whether usage would double if fees were halved.

	Bank has multiple years of record on average credit card balances, payment amounts, credit limit usage, and other key parameters. They create a model to check the impact of the proposed new business policy. The data results show that cutting fees in half for a targetted customer base could increase revenues by $10 million.

What is not data mining?
	1. Expert systems(in artificial intelligence)
		The expert system takes a decision on the experience of designed algorithms
	2. Simple querying
		The query takes a decision according to the given condition in SQL. For example, a database query “SELECT * FROM table” is just a database query and it displays information from the table but actually, this is not hidden information. So it is a simple query and not data mining.

Knowledge Discovery (KDD) Process(Steps In The Data Mining Process):
	The data mining process is divided into two parts i.e. Data Preprocessing and Data Mining. Data Preprocessing involves data cleaning, data integration, data reduction, and data transformation. The data mining part performs data mining, pattern evaluation and knowledge representation of data.

Data Mining Tasks:
	Data mining deals with the kind of patterns that can be mined. On the basis of the kind of data to be mined, there are two categories of functions involved in Data Mining −

	1. Descriptive - focuses on finding human-interpretable patterns describing the data.
	2. Prediction -  involves using some variables or fields in the database to predict unknown or future values of other variables of interest.
	
The goals of prediction and description are achieved by using the following primary data mining tasks:
	- Classification is learning a function that maps (classifies) a data item into one of several predefined classes.
	
	- Regression is learning a function which maps a data item to a real-valued prediction variable.
	
	- Clustering is a common descriptive task where one seeks to identify a finite set of categories or clusters to describe the data.
		Closely related to clustering is the task of probability density estimation which consists of techniques for estimating, from data, the joint multi-variate probability density function of all of the variables/fields in the database.
	
	- Deviation Detection focuses on discovering the most significant changes in the data from previously measured or normative values.
	
	- Sequential Pattern Discovery
	
	- Association Rule Discovery
		Association rule mining finds interesting associations and relationships among large sets of data items. This rule shows how frequently a itemset occurs in a transaction. A typical example is Market Based Analysis.
		
		Market Based Analysis is one of the key techniques used by large relations to show associations between items.It allows retailers to identify relationships between the items that people buy together frequently.

		Example:- Given a set of transactions, we can find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction.
		
		TID	Items
		1	Bread, Milk
		2	Bread, Diaper, Beer, Eggs
		3	Milk, Diaper, Beer, Coke
		4	Bread, Milk, Diaper, Beer
		5	Bread, Milk, Diaper, Coke
		
		Before we start defining the rule, let us first see the basic definitions.
			Support Count(\sigma) – Frequency of occurrence of a itemset.
			Frequent Itemset – An itemset whose support is greater than or equal to minsup threshold.
			Association Rule – An implication expression of the form X -> Y, where X and Y are any 2 itemsets.

		Rule Evaluation Metrics –
			Support(s) –
				The number of transactions that include items in the {X} and {Y} parts of the rule as a percentage of the total number of transaction.It is a measure of how frequently the collection of items occur together as a percentage of all transactions.
				
			Support = \sigma(X+Y) \div total –
				It is interpreted as fraction of transactions that contain both X and Y.
				
			Confidence(c) –
				It is the ratio of the no of transactions that includes all items in {B} as well as the no of transactions that includes all items in {A} to the no of transactions that includes all items in {A}.
				
			Conf(X=>Y) = Supp(X\cupY) \div Supp(X) –
				It measures how often each item in Y appears in transactions that contains items in X also.
				
			From the above table, {Milk, Diaper}=>{Beer}
			s  = \sigma({Milk, Diaper, Beer}) \div |T|
			= 2/5
			= 0.4
			
			c = \sigma(Milk, Diaper, Beer) \div \sigma(Milk, Diaper)
			= 2/3
			= 0.67
			
Association Rule Discovery: Application 
	Let’s look at some areas where Association Rule Mining has helped quite a lot:
	1. Market Basket Analysis:
		This is the most typical example of association mining. Data is collected using barcode scanners in most supermarkets. This database, known as the “market basket” database, consists of a large number of records on past transactions. A single record lists all the items bought by a customer in one sale. Knowing which groups are inclined towards which set of items gives these shops the freedom to adjust the store layout and the store catalog to place the optimally concerning one another.
		
	2. Supermarket shelf management.
		– Goal: To identify items that are bought together by sufficiently many customers.
		– Approach: Process the point-of-sale data collected with barcode scanners to find dependencies among items.
		– A classic rule --
			◆If a customer buys diaper and milk, then he is very likely to buy beer.
			◆So, don’t be surprised if you find six-packs stacked next to diapers!
			
----------------------------------------------------Dr. Pradeep Kumar Class-1-----------------------------------------------------------------------
What is Machine Learning ?
Machine learning is programming computers to optimize a performance criterion using example data or past experience.
	•	Optimize a performance criterion using example data or past experience.
	•	Role of Statistics: Inference from a sample
	•	Role of Computer science: Efficient algorithms to • Solve the optimization problem
	•	Representing and evaluating the model for inference

Learning is used when:
	• Human expertise does not exist (navigating on Mars),
	• Humans are unable to explain their expertise (speech recognition) 
	• Solution changes in time (routing on a computer network)
	• Solution needs to be adapted to particular cases (user biometrics)
--------------------------------------- few case studies of ML, how it bring some changes to organisation -------------------
1. Machine Learning Case Study on Dell:
	The multinational leader in technology, Dell, empowers people and communities from across the globe with superior software and hardware. Since data is a core part of Dell’s hard drive, their marketing team needed a data-driven solution that supercharges response rates and displays why certain words and phrases outperform others.

	Dell partnered with Persado, the world’s leading technology in AI and ML generated marketing creative, to harness the power of words in their email channel and garner data-driven analytics for each of their key audiences.

	As a result of this partnership, Dell noticed a 50% average increase in CTR and a 46% average increase in responses from customers. It also generated a 22% average increase in page visits and a 77% average increase in add-to-carts.

	Excited by their success and learnings with email, Dell was eager to elevate their entire marketing platform with Persado. Dell now uses machine learning to improve the marketing copy of their promotional and lifecycle emails, Facebook ads, display banners, direct mail, and even radio content.
	
2. Machine Learning Case Study on Sky:
	Sky UK transforms customer experiences with the help of machine learning and artificial intelligence through Adobe Sensei.

	“We have 22.5 million very diverse customers. Even attempting to divide people by their favorite television genre can result in pretty broad segments.” said the Head of Digital Decisioning and Analytics, Sky UK.

	This will:
		• Create hyper-focused segments to engage customers.
		• Use machine learning to deliver actionable intelligence.
		• Improve relationships with customers.
		• Apply AI learnings across channels to understand what matters to customers.
		• The company was able to make sense of its large volumes of customer information with the help of machine learning frameworks to recommend them with products and services that resonated the most with each customer.

	“People think of machine learning as a tool for delivering experiences that are strictly defined and very robotic, but it’s actually the opposite. With Adobe Sensei, we’re drawing a line that connects customer intelligence and personalized experiences that are valuable and appropriate” says McLaughlin.
	
3. Machine Learning Case Study on Trendyol:
Trendyol which is a leading e-commerce company based in Turkey faced threat from global competitors like Adidas and ASOS, particularly for sportswear.

To help gain customer loyalty and enhance its emailing system, it partnered with vendor Liveclicker, which specializes in real-time personalization.

Trendyol used machine learning and artificial intelligence to create several highly personalized marketing campaigns. It also helped to distinguish which messages would be most relevant to which customers. It also created an offer for a football jersey imposing the recipient’s name on the back to ramp up personalization.

By creatively using one-to-one personalization, the retailer’s open rates, click-through rates, conversions, and sales reached all-time highs. It generated a 30% increase in click-through rates for Trendyol, a 62% growth in response rates, and an impressive 130% increase in conversion rates.

It has now also employed strong marketing functions like social media utilization, mobile app, SEO blogs, celebrity endorsement, etc to reach its customer base.

4. Machine Learning Case Study on Harley Davidson:
The place we are in today is where it is difficult to break through traditional marketing. For a business like – Harley Davidson NYC, Albert (an artificial intelligence-powered robot) has a lot of appeal. Powered by machine learning and artificial intelligence, robots are writing news stories, working in hotels, managing traffic, and even running McDonald’s.

Albert can be applied to various marketing channels including social media and email. The software predicts which consumers are most likely to convert and adjusts personal creative copies on its own.

Harley Davidson is the only brand to make use of Albert. The company analyzed customer data to determine the behavior of previous customers whose actions were positive in terms of purchasing and spending more than the average amount of time on browsing through the website. With this information, Albert created segments of customers and scaled up the test campaigns accordingly.

Results show that Harley Davidson increased its sales by 40% with the use of Albert. The brand also had a 2,930% increase in leads, with 50% of those from high converting ‘lookalikes’ identified by artificial intelligence and machine learning.

5. Machine Learning Case Study on Yelp:
While Yelp might not seem to be a tech company at first glance, it is taking advantage of machine learning to improve users’ experience.

Yelp’s machine learning algorithms help the company’s human staff to collect, categorize, and label images more efficiently. Since images are almost as vital to Yelp as user reviews themselves, it is always trying to improve how it handles image processing. Through this, the company is serving millions of its users now.

For an entire generation today, taking photos of their food has become second nature and thanks to these people because of whom Yelp has such a huge database of photos. Its software uses techniques for analysis of the image to identify color, texture, and shape. It means that it can recognize the presence of say, pizzas, or whether a restaurant has outdoor seating.

As a result, the company is now able to predict attributes like ‘good for kids’ and ‘classy ambiance’ with more than 80% accuracy. It is also planning to use this information to auto-caption images and improve search recommendations in the future.
	
---------------------------------------------------------------------------------------------------------------------------------

Types of Learning 
• Supervised Learning
	 • Classification	
	 • Regression 
• Unsupervised Learning 
	• Association 
	• Clustering 
• Reinforcement Learning 
	• Markov Model	
	• Q Learning 

Difference b/w Analytics and Statistics 
	• Analytics deals deal with what we know Statistics deal with what we don’t know 
	• Analytics is looking and making inference from facts Statistics is learning beyond the facts 
	• Analytics deals with certainity Statistics deal with uncertainity 
	• Analytics require coding skills

Difference between the application-oriented and subject-oriented organization of data.
	• Subject-oriented programming is an object-oriented approach in which different subsystems known as subjects are divided to create new subjects based on the composition expression. 

	Lets Break this down:
		How do you make software that allows a number of people to work together on the same group of things? Think of air traffic control, logistics planning, multi user games, , or even software development itself. In each of these examples, there is both a group of people and a set of the things they're working on, and what we really want to do is create a little universe in which they can all exist and interact with each other in real time. This little universe, of course, isn't new -- it's been accomplished in many ways in many different situations. But at some point we must ask ourselves if there is a more overarching way to solve this recurring problem. 
	The approach is a radical departure from the classical object-oriented approach, in which objects are defined based on their properties and methods. Subject-oriented programming is largely oriented toward dividing an object-oriented system into subjects. It thus provides a compositional view of the application development.

	Example:
	A. "Object-oriented": to break menu into pieces, containing operations with objects:
	Code:
		.	--------------------------------- 
		.	   RESUMES  |   VACANCIES   | ... 
		.	 search add | search    add | ... 
		.	 all     my | all        my | ... 
	.	--------------------------------- 

	B. "Subject-oriented": to break menu into pieces by type of site's visitor:
	Code:
		.	--------------------------------- 
		.	  JOB-SEEKERS    |  EMPLOYERS 
		.	 All vacancies   | All resumes 
		.	 Searh vacancies | Search resumes 
		.	 Add resume      | Add vacancy 
		.	 My resumes      | My vacancies 
		.	  ...            |    ... 
		.	--------------------------------- 

Q. What is a data lake?
A data lake is a storage repository that holds a vast amount of raw data in its native format until it is needed for analytics applications. While a traditional data warehouse stores data in hierarchical dimensions and tables, a data lake uses a flat architecture to store data, primarily in files or object storage. That gives users more flexibility on data management, storage and usage. Data lakes are often associated with Hadoop systems. In deployments based on the distributed processing framework, data is loaded into the Hadoop Distributed File System (HDFS) and resides on the different computer nodes in a Hadoop cluster.
What makes data lake successful? ——— Right platform + Right Data + Right interface

SEMMA Model:
SEMMA is the sequential methods to build machine learning models incorporated in ‘SAS Enterprise Miner’
Sample — This step entails choosing a subset of the appropriate volume dataset from a vast dataset that has been given for the model’s construction.
Explore — During this step, univariate and multivariate analysis is conducted in order to study interconnected relationships between data elements and to identify gaps in the data. 
Modify — In this step, lessons learned in the exploration phase from the data collected in the sample phase are derived with the application of business logic. 
Model — With the variables refined and data cleaned, the modeling step applies a variety of data mining techniques in order to produce a projected model of how this data achieves the final, desired outcome of the process.
Assess — In this final SEMMA stage, the model is evaluated for how useful and reliable it is for the studied topic. The data can now be tested and used to estimate the efficacy of its performance.

Case study on SEMMA model and k-mean clustering ------ https://github.com/gkmishragaurav/IIML_Notes/blob/main/PA%26AI/socsci-10-00004-1.pdf

CRISP-DM (CRoss‐Industry Standard Process for Data Mining) — is a process model with six phases that naturally describes the data science life cycle. It’s like a set of guardrails to help you plan, organize, and implement your data science (or machine learning) project.
For more detail reading — https://www.datascience-pm.com/crisp-dm-2/
	1.	Business understanding – What does the business need?
	2.	Data understanding – What data do we have / need? Is it clean?
	3.	Data preparation – How do we organize the data for modeling?
	4.	Modeling – What modeling techniques should we apply?
	5.	Evaluation – Which model best meets the business objectives?
	6.	Deployment – How do stakeholders access the results?
	
The analytical DELTA: is an acronym for the five success factors that you need to have a successful analytics program or project in your organization, so that you can compete in the market by data powered decision-making.
DELTA covers the following five success factors:
	•	D for accessible, high quality data
	•	E for an enterprise orientation
	•	L for analytical leadership
	•	T for strategic targets
	•	A for analysts
	
Five Stages of Analytics Maturity:
Organizations mature their analytical capabilities as they develop in the seven areas. The maturity model, described in Competing on Analytics and developed in Analytics at Work, helps companies measure their growth across the DELTA elements. 

	Stage 1: Analytically Impaired — These companies rely primarily on gut feel to make decisions, and they have no formal plans for becoming more analytical.
	Stage 2: Localized Analytics. ——Analytics or reporting at these companies exist within silos. There is no means or structure for collaborating across organizational units or functions in the use of analytics. 
	Stage 3: Analytical Aspirations. —These companies see the value of analytics, and intend to improve their capabilities for generating and using them. Thus far, however, they have made little progress in doing so.
	Stage 4: Analytical Companies. — Companies in this category are good at multiple aspects of analytics. They are highly data-oriented, have analytical tools and make wide use of analytics with some coordination across the organization. However there remains a lack of commitment to fully compete on analytics or use them strategically.
	Stage 5: Analytical Competitors. These companies use analytics strategically and pervasively across the entire enterprise. They view their analytical capabilities as a competitive weapon, and they already seen some competitive advantage result from analytics.

Key roles in successful analytics project 
	• Business user – Understands the domain Area 
	• Project Sponsor ‐‐ Provides requirements 
	• Project Manager ‐‐ Ensures Meeting Objectives 
	• Business Intelligence analysts ‐‐ Provides business domain expertise based on deep understanding of the data
	• Database Administrator ‐‐ Creates DB Environment 
	• Data engineer ‐‐ provides technical skills, assists data management and extraction, supports analytical activitie
	• Data Scientist ‐‐ Provides analytic techniques and modeling.
Let say one want to make a data team, what should be the approach and mindset ---- https://towardsdatascience.com/how-to-choose-the-right-structure-for-your-data-team-be6c1b66a067

Data Analytics Lifecycle :
The Data analytic lifecycle is designed for Big Data problems and data science projects. The cycle is iterative to represent real project. To address the distinct requirements for performing analysis on Big Data, step – by – step methodology is needed to organize the activities and tasks involved with acquiring, processing, analyzing, and repurposing data.

Phase 1: Discovery –
•	The data science team learn and investigate the problem.
•	Develop context and understanding.
•	Come to know about data sources needed and available for the project.
•	The team formulates initial hypothesis that can be later tested with data.

Phase 2: Data Preparation –
•	Steps to explore, preprocess, and condition data prior to modeling and analysis.
•	It requires the presence of an analytic sandbox, the team execute, load, and transform, to get data into the sandbox.
•	Data preparation tasks are likely to be performed multiple times and not in predefined order.
•	Several tools commonly used for this phase are – Hadoop, Alpine Miner, Open Refine, etc.

Phase 3: Model Planning –
•	Team explores data to learn about relationships between variables and subsequently, selects key variables and the most suitable models.
•	In this phase, data science team develop data sets for training, testing, and production purposes.
•	Team builds and executes models based on the work done in the model planning phase.
•	Several tools commonly used for this phase are – Matlab, STASTICA.

Phase 4: Model Building –
•	Team develops datasets for testing, training, and production purposes.
•	Team also considers whether its existing tools will suffice for running the models or if they need more robust environment for executing models.
•	Free or open-source tools – Rand PL/R, Octave, WEKA.
•	Commercial tools – Matlab , STASTICA.

Phase 5: Communication Results –
•	After executing model team need to compare outcomes of modeling to criteria established for success and failure.
•	Team considers how best to articulate findings and outcomes to various team members and stakeholders, taking into account warning, assumptions.
•	Team should identify key findings, quantify business value, and develop narrative to summarize and convey findings to stakeholders.

Phase 6: Operationalize –
•	The team communicates benefits of project more broadly and sets up pilot project to deploy work in controlled way before broadening the work to full enterprise of users.
•	This approach enables team to learn about performance and related constraints of the model in production environment on small scale  , and make adjustments before full deployment.
•	The team delivers final reports, briefings, codes.
•	Free or open source tools – Octave, WEKA, SQL, MADlib.

Factors causing failures:
• Improper planning
• Inadequate project management
• Company not ready for a data warehouse 
• Insufficient staff training
• Improper team management
• No support from top management 

 Application Areas
• E‐Commerce and Market Research 
• E‐Government and Politics
• Science and Technology
• Smart Health and wellbeing
• Security and Public Safety
