Business Analytics (BA) – is the act of bringing quantitative data to bear on decision making.
In simple terms --- is data visualization and reporting for understand “what happened and what is happening.
Benefits:
1. More Informed Decision-Making - data-driven decision-making
2. Improved Operational Efficiency

How BI based decision should be taken?
	A business decision should always be backed by data
	How to Collect this data?  ---- By identifying right KPIs
	What is a good KPI for business analysts?
		Agreed - all stakeholders should agree on the business analysts’ KPIs.
		Calibrated - scaled with expectations, scope and goals.
		Proper to the business environment - a KPI should be applicable to the relevant context of usage 
		Relevant to the assessment purpose - a KPI target purpose should be clearly stated - to predict  or to track  the business analysts’ work.
		Weighted – KPIs should have its own weight and priority may not weigh equally.
	
	How often the KPIs should be consumed and refined?
	There are two main cycles of consuming and refining KPIs:
		each deliverable should end up with retrospection of the achieved
		
		
automation vs autonomous:
	Automation refers to the ability of a system to control. Ex- a vehicle, like autopilot or cruise control.
	Autonomous is the ability of a system to not only control a vehicle but respond to unexpected hazards. An autonomous system, learns and adapts to dynamic environments, and evolves as the environment around it changes. 
	
4 phases of data analytics:
	analytics 1.0
	big data - 2.0
	big data + IOT - 3.0
	Autonomous - 4.0
	
Q. How to do data analytics - By gathering data --- which should be based on market study

Read case study - 
	CDMA + relience 
	AArogya setu app 
	
Descriptive analysis:
	- What happeded?
	- what is happening?
	Tools:
		Business reporting
		dashboard
		scoreboard
		data warehousing
	Outcome:
		Well defined business problems and opportunities.
		
Predictive analysis:
	- What will happen?
	- Why will it happen?
	Tools:
		Data Mining
		Text mining
		forecasting
	Outcome:
		Accurate projections of future events and outcomes.
		
Prescriptive analytics:
	- making likey forecasting
	- What we do moving forward?
	Tools:
		Optimization
		simulation
		decision modeling
		expert systems
	Outcome:
		Best possible decision and action

What is Data Warehousing?
	A Data Warehousing (DW) is process for collecting and managing data from varied sources to provide meaningful business insights.
	A Data warehouse is typically used to connect and analyze business data from heterogeneous sources. 

Properties of data warehousing:
	1. subject oriented - data warehousing process is proposed to handle with a specific theme which is more defined. These themes can be sales, distributions, marketing etc.
	2. Integrated – It is somewhere same as subject orientation which is made in a reliable format.  Integration means founding a shared entity to scale the all similar data from the different databases. The data also required to be resided into various data warehouse in shared and generally granted manner.
	3. Time-Variant – In this data is maintained via different intervals of time such as weekly, monthly, or annually etc. It founds various time limit which are structured between the large datasets and are held in online transaction process (OLTP). The time limits for data warehouse is wide-ranged than that of operational systems.
	4. Non-Volatile – As the name defines the data resided in data warehouse is permanent. It also means that data is not erased or deleted when new data is inserted. It includes the mammoth quantity of data that is inserted into modification between the selected quantity on logical business. It evaluates the analysis within the technologies of warehouse.
	
language used for data manupulation called - DML - data manupulation language 

Database vs data warehouse
	- Database is a collection of related data that represents some elements of the real world whereas Data warehouse is an information system that stores historical and commutative data from single or multiple sources.
	- Database is designed to record data whereas the Data warehouse is designed to analyze data.
	- Database is application-oriented-collection of data whereas Data Warehouse is the subject-oriented collection of data.
	- Database uses Online Transactional Processing (OLTP) whereas Data warehouse uses Online Analytical Processing (OLAP).
	- Database tables and joins are complicated because they are normalized whereas Data Warehouse tables and joins are easy because they are denormalized.
	- ER modeling techniques are used for designing Database whereas data modeling techniques are used for designing Data Warehouse.
	
oltp vs opal:
	- oltp is an online transactional system. It manages database modification. OLAP is an online analysis and data retrieving process.
	- OLAP creates a single platform for all type of business analysis needs which includes planning, budgeting, forecasting, and analysis while OLTP is useful to administer day to day transactions of an organization.
	- OLAP is characterized by a large volume of data while OLTP is characterized by large numbers of short online transactions.
	- In OLAP, data warehouse is created uniquely so that it can integrate different data sources for building a consolidated database whereas OLTP uses traditional DBMS.
	
Pyramid Diagram of Organizational levels and information requirements:
	for detail reading - https://www.guru99.com/mis-types-information-system.html
	3layers:
	top layer - ESS - Executive support system - this data always unstructured 
	middle layer - MIS/DIS - management support system/decision support system - data is semi structured.
	bottom layer - TPS - Transaction Processing System - data is highly structured

What is OLAP?
	Online Analytical Processing (OLAP) is a category of software that allows users to analyze information from multiple database systems at the same time. It is a technology that enables analysts to extract and view business data from different points of view.
		
OLAP cube:
	for details - 
	https://www.guru99.com/online-analytical-processing.html
	https://galaktika-soft.com/blog/olap-cubes.html
	1. At the core of the OLAP concept, is an OLAP Cube. The OLAP cube is a data structure optimized for very quick data analysis.
	2. The OLAP Cube consists of numeric facts called measures which are categorized by dimensions. OLAP Cube is also called the hypercube.
	3. Usually, data operations and analysis are performed using the simple spreadsheet, where data values are arranged in row and column format. This is ideal for two-dimensional data. However, OLAP contains multidimensional data, with data usually obtained from a different and unrelated source. Using a spreadsheet is not an optimal option. The cube can store and analyze multidimensional data in a logical and orderly manner.
	
How does it work?
	A Data warehouse would extract information from multiple data sources and formats like text files, excel sheet, multimedia files, etc.
	The extracted data is cleaned and transformed. Data is loaded into an OLAP server (or OLAP cube) where information is pre-calculated in advance for further analysis.	
	
Basic analytical operations of OLAP
	Four types of analytical OLAP operations are:
		1. Roll-up
		2. Drill-down
		3. Slice 
		4. dice
		
	1) Roll-up:
	Roll-up is also known as “consolidation” or “aggregation.” The Roll-up operation can be performed in 2 ways
		1. Reducing dimensions
		2. Climbing up concept hierarchy. Concept hierarchy is a system of grouping things based on their order or level.
		
	NOTE: In the roll-up process at least one or more dimensions need to be removed.

	2) Drill-down:
	In drill-down data is fragmented into smaller parts. It is the opposite of the rollup process. It can be done via
		- Moving down the concept hierarchy
		- Increasing a dimension

	NOTE: In the roll-up process at least one or more dimensions need to be added.
	
	3) Slice:
	Here, one dimension is selected, and a new sub-cube is created. achieved by cutting horizontally.
	
	4) Dice:
	This operation is similar to a slice. The difference in dice is you select 2 or more dimensions that result in the creation of a sub-cube.
	
What is Data Mining?
	More details - https://www.guru99.com/data-mining-tutorial.html
	- Data Mining is a process of finding potentially useful patterns from huge data sets. It is a multi-disciplinary skill that uses machine learning, statistics, and AI to extract information to evaluate future events probability. The insights derived from Data Mining are used for marketing, fraud detection, scientific discovery, etc. 
	- Data Mining is all about discovering hidden, unsuspected, and previously unknown yet valid relationships amongst the data. Data mining is also called Knowledge Discovery in Data (KDD), Knowledge extraction, data/pattern analysis, information harvesting, etc.
	
Difference Between Data Mining and Statistics:

	Data mining: Data mining is the method of analyzing expansive sums of data in an exertion to discover relationships, designs, and insights. These designs, concurring to Witten and Eibemust be “meaningful in that they lead to a few advantages, more often than not a financial advantage.” Data in data mining is additionally ordinarily quantitative particularly when we consider the exponential development in data delivered by social media later a long time, i.e. big-data.

	Statistics: Statistics is the science of collecting, organizing, summarizing, and analyzing data to draw conclusions or reply questions. In expansion, measurements are around giving a degree of certainty in any conclusions. The practice or science of collecting and analyzing numerical information in huge amounts, particularly for the reason of gathering extents in entirety from those in a representative test.

Data mining Examples:
	Now in this Data Mining course, let’s learn about Data mining with examples:

	Example 1:

	Consider a marketing head of telecom service provides who wants to increase revenues of long distance services. For high ROI on his sales and marketing efforts customer profiling is important. He has a vast data pool of customer information like age, gender, income, credit history, etc. But its impossible to determine characteristics of people who prefer long distance calls with manual analysis. Using data mining techniques, he may uncover patterns between high long distance call users and their characteristics.

	For example, he might learn that his best customers are married females between the age of 45 and 54 who make more than $80,000 per year. Marketing efforts can be targeted to such demographic.

	Example 2:

	A bank wants to search new ways to increase revenues from its credit card operations. They want to check whether usage would double if fees were halved.

	Bank has multiple years of record on average credit card balances, payment amounts, credit limit usage, and other key parameters. They create a model to check the impact of the proposed new business policy. The data results show that cutting fees in half for a targetted customer base could increase revenues by $10 million.

What is not data mining?
	1. Expert systems(in artificial intelligence)
		The expert system takes a decision on the experience of designed algorithms
	2. Simple querying
		The query takes a decision according to the given condition in SQL. For example, a database query “SELECT * FROM table” is just a database query and it displays information from the table but actually, this is not hidden information. So it is a simple query and not data mining.

Knowledge Discovery (KDD) Process(Steps In The Data Mining Process):
	The data mining process is divided into two parts i.e. Data Preprocessing and Data Mining. Data Preprocessing involves data cleaning, data integration, data reduction, and data transformation. The data mining part performs data mining, pattern evaluation and knowledge representation of data.

Data Mining Tasks:
	Data mining deals with the kind of patterns that can be mined. On the basis of the kind of data to be mined, there are two categories of functions involved in Data Mining −

	1. Descriptive - focuses on finding human-interpretable patterns describing the data.
	2. Prediction -  involves using some variables or fields in the database to predict unknown or future values of other variables of interest.
	
The goals of prediction and description are achieved by using the following primary data mining tasks:
	- Classification is learning a function that maps (classifies) a data item into one of several predefined classes.
	
	- Regression is learning a function which maps a data item to a real-valued prediction variable.
	
	- Clustering is a common descriptive task where one seeks to identify a finite set of categories or clusters to describe the data.
		Closely related to clustering is the task of probability density estimation which consists of techniques for estimating, from data, the joint multi-variate probability density function of all of the variables/fields in the database.
	
	- Deviation Detection focuses on discovering the most significant changes in the data from previously measured or normative values.
	
	- Sequential Pattern Discovery
	
	- Association Rule Discovery
		Association rule mining finds interesting associations and relationships among large sets of data items. This rule shows how frequently a itemset occurs in a transaction. A typical example is Market Based Analysis.
		
		Market Based Analysis is one of the key techniques used by large relations to show associations between items.It allows retailers to identify relationships between the items that people buy together frequently.

		Example:- Given a set of transactions, we can find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction.
		
		TID	Items
		1	Bread, Milk
		2	Bread, Diaper, Beer, Eggs
		3	Milk, Diaper, Beer, Coke
		4	Bread, Milk, Diaper, Beer
		5	Bread, Milk, Diaper, Coke
		
		Before we start defining the rule, let us first see the basic definitions.
			Support Count(\sigma) – Frequency of occurrence of a itemset.
			Frequent Itemset – An itemset whose support is greater than or equal to minsup threshold.
			Association Rule – An implication expression of the form X -> Y, where X and Y are any 2 itemsets.

		Rule Evaluation Metrics –
			Support(s) –
				The number of transactions that include items in the {X} and {Y} parts of the rule as a percentage of the total number of transaction.It is a measure of how frequently the collection of items occur together as a percentage of all transactions.
				
			Support = \sigma(X+Y) \div total –
				It is interpreted as fraction of transactions that contain both X and Y.
				
			Confidence(c) –
				It is the ratio of the no of transactions that includes all items in {B} as well as the no of transactions that includes all items in {A} to the no of transactions that includes all items in {A}.
				
			Conf(X=>Y) = Supp(X\cupY) \div Supp(X) –
				It measures how often each item in Y appears in transactions that contains items in X also.
				
			From the above table, {Milk, Diaper}=>{Beer}
			s  = \sigma({Milk, Diaper, Beer}) \div |T|
			= 2/5
			= 0.4
			
			c = \sigma(Milk, Diaper, Beer) \div \sigma(Milk, Diaper)
			= 2/3
			= 0.67
			
Association Rule Discovery: Application 
	Let’s look at some areas where Association Rule Mining has helped quite a lot:
	1. Market Basket Analysis:
		This is the most typical example of association mining. Data is collected using barcode scanners in most supermarkets. This database, known as the “market basket” database, consists of a large number of records on past transactions. A single record lists all the items bought by a customer in one sale. Knowing which groups are inclined towards which set of items gives these shops the freedom to adjust the store layout and the store catalog to place the optimally concerning one another.
		
	2. Supermarket shelf management.
		– Goal: To identify items that are bought together by sufficiently many customers.
		– Approach: Process the point-of-sale data collected with barcode scanners to find dependencies among items.
		– A classic rule --
			◆If a customer buys diaper and milk, then he is very likely to buy beer.
			◆So, don’t be surprised if you find six-packs stacked next to diapers!
